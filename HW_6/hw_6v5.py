# -*- coding: utf-8 -*-
"""HW_6v5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f13fmP4wUeSn03M_Mr7Svl_Q7Dst3oCi
"""

import pandas as pd

try:
      file_path = 'Truck_sales.csv'
      df_truck = pd.read_csv(file_path)
      print(f"Файл {file_path} загружен")

except Exception as e: # Catching a more general exception
      print(f"Ошибка загрузки файла: {e}")

#Dataset: https://www.kaggle.com/datasets/ddosad/dummy-truck-sales-for-time-series
#Приведенный ниже набор данных содержит ежемесячные данные о продажах грузовиков определенной компании в течение года.
#Подозреваю что набор синтетический, но в принципе это не имеет значения, пусть это будет США и какой-нибудь Freightliner Cascadia
#Состав dataset'а:
#Month-Year: месяц и год продажи
#Number_Trucks_Sold: количество проданных грузовиков

df_truck.head(10)

#Информация о dataset
df_truck.info()

df_truck.describe()

#Пустые значения - отсутствуют
print(df_truck.isnull().sum())

#Добавим столбец sales_date: "Month-Year" в формате datetime
df_truck['sales_date'] = pd.to_datetime(df_truck['Month-Year'], format='%y-%b')

#Дополнительно столбец с месяцем и столбец с годом
df_truck['sales_year'] = df_truck['sales_date'].dt.year
df_truck['sales_month'] = df_truck['sales_date'].dt.month

df_truck.head(10)

#Проверим по всем ли месяцам есть данные
full_range = pd.date_range(start=df_truck['sales_date'].min(), freq='MS', end=df_truck['sales_date'].max()) #MS - первое число месяца

df_full = pd.DataFrame({'full_date': full_range}) # Создание DataFrame со всеми месяцами диапазона

#Слияние DataFrames
df_truck = df_full.merge(df_truck[['sales_date','sales_year','sales_month','Number_Trucks_Sold']], left_on='full_date', right_on='sales_date', how = 'left')

#Если диапазон в файле был полностью заполнен - то пустых значений не будет, так как их не было изначально, иначе  - будут
print(df_truck.isnull().sum())

#Переименуем столбец
df_truck.rename(columns={'Number_Trucks_Sold':'truck_sales'}, inplace=True)

#Индекс
df_truck = df_truck.set_index('full_date')

#сортировка по дате
df_truck.sort_values(by='sales_date', inplace=True)

vts = df_truck['truck_sales'] # перменная что бы покороче

#Посмотрим что с данными
monthly_sales_data = pd.pivot_table(df_truck, values = "truck_sales", columns = "sales_year", index = "sales_month")
monthly_sales_data

# Commented out IPython magic to ensure Python compatibility.
# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

sns.set(style="white", rc={'figure.figsize': (15, 4)})
ax = monthly_sales_data.plot()
ax.legend(loc='upper left', bbox_to_anchor=(1, 1))
ax.set_facecolor('white')
plt.show()

sns.set(style="white", rc={'figure.figsize': (15, 4)})
monthly_sales_data.boxplot()
plt.show()

yearly_sales_data = pd.pivot_table(df_truck, values = "truck_sales", columns = "sales_month", index = "sales_year")
yearly_sales_data

sns.set(style="white", rc={'figure.figsize': (10, 4)})
ax = yearly_sales_data.plot()
ax.legend(loc='upper left', bbox_to_anchor=(1, 1))
ax.set_facecolor('white')
plt.show()

sns.set(rc={'figure.figsize': (10, 4)})
yearly_sales_data.boxplot()
plt.show()

#Выводы
#Продажи грузовиков растут с каждым годом.
#Июль/август — пиковые месяцы продаж.

# Аномалии (через Isolation Forest)
from sklearn.ensemble import IsolationForest

iso = IsolationForest(contamination=0.01, random_state=42)# 1% - аномальных данных
df_truck['an'] = iso.fit_predict(df_truck[['truck_sales']])
anomalies = df_truck[df_truck['an'] == -1]

plt.figure(figsize=(12,5))
plt.plot( df_truck['truck_sales'], label='Количество проданных')
plt.scatter(anomalies['sales_date'], anomalies['truck_sales'], color='red', label='Аномалия')
plt.legend()
plt.title('Аномалии в данных')
plt.show()

#Надо было найти 1% ошибок, вот и нашли

#Оценим свойства time series

#Тест Дики-Фуллера
import numpy as np
from statsmodels.tsa.stattools import adfuller

result = adfuller(df_truck['truck_sales'])
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))

if result[0]> result[4]['5%']:
    print ('есть единичные корни, ряд не стационарен')
else:
    print ('единичных корней нет, ряд стационарен')

#Результаты теста Дики-Фуллера для не стационарного временного ряда:
# ADF статистика: 1.115893. Положительное значение статистики - признак нестационарности.
# p-значение: 0.995350. Значение значительно выше 0.05, не отвергаем гипотезу о нестационарности ряда.
# Критические значения для разных уровней значимости показывают, что статистика ADF выше этих значений, что также подтверждает нестационарность ряда.

#Декомпозиция ряда
import statsmodels.api as sm
decomposition = sm.tsa.seasonal_decompose(vts, model='multiplicative')

fig = decomposition.plot()
fig.set_figwidth(10)
fig.set_figheight(8)
fig.suptitle('Декомпозиция')
plt.show()

#Тренд: 12-месячная скользящая средняя представляет собой довольно прямую линию, что указывает на линейный тренд.
#Сезонность: 12-месячная сезонность отчетливо видна.
#Шум - случайный: мультипликативная модель работает, поскольку в остатках нет закономерностей.

#График 1: Ряд данных, который был разложен.
#График 2: Составляющая тренда
#График 3: Сезонная составляющая ряда данных.
#График 4: Остаточный компонент ряда данных - шум

import warnings
warnings.filterwarnings("ignore")

import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

# Визуализируем временной ряд
plt.figure(figsize=(10, 6))
plt.plot(df_truck['sales_date'], df_truck['truck_sales'])
plt.title("Количество проданных автомобилей (исходные данные)")
plt.xlabel("Дата")
plt.ylabel("Значение")
plt.grid(True)
plt.show()

# Автокорреляция
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
plt.figure(figsize=(10, 4))
plot_acf(df_truck['truck_sales'], lags=40)
plt.title('Автокорреляция количества проданных грузовиков')
plt.show()

#Функция plot_acf используется для визуализации функции автокорреляции (ACF) временного ряда.
#ACF измеряет корреляцию между временным рядом и его лагированными версиями.
#Этот график является основополагающим инструментом анализа временных рядов для выявления закономерностей и
#определения порядка компонентов скользящего среднего (MA) в моделях, таких как ARIMA.

#Бледно-голубая фигура демонстрирует величину задержки между первым и каждым последующим наблюдением, потому высота ее растет по мере перехода к новому наблюдению.
#Столбцы с шарообразными наконечниками — это своеобразная визитная карточка коррелограммы, высота столбца отображает степень корреляции.

#Для части моделей, которыми я буду пользоваться, необходим стационарные временной ряд

#Выполнение дифференциации (d=1), поскольку данные нестационарны:
plt.figure(figsize=(15, 4))
plt.plot(vts.diff(periods=1))
plt.xlabel('Years')
plt.ylabel('Truck Sales')

#Попробуем для d=2
plt.figure(figsize=(15, 4))
plt.plot(vts.diff(periods=2))
plt.xlabel('Years')
plt.ylabel('Truck Sales')

#Мы наблюдаем сезонность даже после дифференцирования. Это предполагает логарифмическое преобразование данных.
plt.figure(figsize=(15, 4))
plt.plot(np.log10(vts))
plt.xlabel('Years')
plt.ylabel('Log (Truck Sales)')

#Мы наблюдаем тренд и сезонность даже после логарифмирования наблюдений. Попробуем совместить для d=1
plt.figure(figsize=(15, 5))
plt.plot(np.log10(vts).diff(periods=1))
plt.xlabel('Years')
plt.ylabel('Differenced Log (Truck Sales)')

#Для d=2
plt.figure(figsize=(15, 5))
plt.plot(np.log10(vts).diff(periods=2))
plt.xlabel('Years')
plt.ylabel('Differenced Log (Truck Sales)')

#Тест Дики-Фуллера
vts_static = np.log10(vts).diff(periods=2)
vts_static=vts_static.dropna()

result = adfuller(vts_static)
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))
if result[0]> result[4]['5%']:
    print ('есть единичные корни, ряд не стационарен')
else:
    print ('единичных корней нет, ряд стационарен')

#Результаты теста Дики-Фуллера для не стационарного временного ряда:
# ADF статистика: -3.210712. Отрицательно значение статистики - признак стационарности.
# p-значение: 0.019373. Значение меньше выше 0.05.
# Критические значения для разных уровней значимости показывают, что статистика ADF выше этих значений, что также подтверждает нестационарность ряда.

#Модель номер 1: ARMA Model

#Улучшение моделей авторегрессии с помощью прогнозов на основе скользящего среднего.
#Модели ARMA состоят из двух компонентов:
#Модель AR: данные моделируются на основе прошлых наблюдений.
#Модель MA: ошибки предыдущих прогнозов учитываются в модели.

#Разделим данные train and test
df_truck['date'] = df_truck.index
train = df_truck[df_truck.index < '2013-01-01']
test = df_truck[df_truck.index >= '2013-01-01']

dftest = adfuller(train['truck_sales'])#Проверим на стационарность
dftest
print('DF test statistic is %3.3f' %dftest[0])
print('DF test p-value is %1.4f' %dftest[1])

train_sales_ts_log = np.log10(train['truck_sales'])
#Логарифмическое преобразование обучающих данных для того, чтобы сделать временной ряд стационарным, как мы это делали с полными данными

!pip install -q pmdarima

from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error, mean_squared_error

#ARMA Model building to estimate best 'p' , 'q' ( Lowest AIC Approach )
#тестировались варианты [(1, 0, 1), (1, 0, 2), (1, 0, 3), (2, 0, 1), (2, 0, 2), (2, 0, 3), (3, 0, 1), (3, 0, 2), (3, 0, 3)]
ARIMA_model = ARIMA(train_sales_ts_log,order = (3,1,3))
ARIMA_result = ARIMA_model.fit()

print(ARIMA_result.summary().tables[0])
print(ARIMA_result.summary().tables[1])

#Подсчет RMSE модели
pred_dynamic = ARIMA_result.get_prediction(start=pd.to_datetime('2012-01-01'), dynamic=True, full_results=True)
pred99 = ARIMA_result.get_forecast(steps=len(test), alpha=0.1)#forecasting values

# Extract the predicted and true values of our time series
truck_sales_forecasted = pred_dynamic.predicted_mean
testCopy1 = test.copy()
testCopy1['truck_sales_forecasted'] = np.power(10, pred99.predicted_mean)

testCopy1

mse_ARMA = ((testCopy1['truck_sales'] - testCopy1['truck_sales_forecasted']) ** 2).mean()
rmse_ARMA = np.sqrt(mse_ARMA)
print('The Root Mean Squared Error ARMA {}'.format(round(rmse_ARMA, 4)))

#Визуализация
axis = train['truck_sales'].plot(label='Обучение', figsize=(15, 5))
testCopy1['truck_sales'].plot(ax=axis, label='Тест', alpha=0.7)
testCopy1['truck_sales_forecasted'].plot(ax=axis, label='Предсказано', alpha=0.7)
axis.set_xlabel('Дата')
axis.set_ylabel('Проданные грузовики')
plt.legend(loc='best')
plt.show()
plt.close()

#Модель SARIMA
#Модели ARIMA можно расширить/улучшить для обработки сезонных компонентов ряда данных.

SARIMA_model = sm.tsa.statespace.SARIMAX(train_sales_ts_log,
                                         order=(1, 0, 1),
                                         seasonal_order=(1, 0, 1, 12),
                                         enforce_stationarity=True)
#Здесь:
#- p = порядок несезонной авторегрессии = 1,
#- d = несезонная дифференциация = 0,
#- q = порядок несезонной скользящей средней = 1,
#- P = порядок сезонной авторегрессии = 1,
#- D = сезонная дифференциация = 0,
#- Q = порядок сезонной скользящей средней = 1,
#- S = временной интервал повторяющегося сезонного паттерна = 12.
SARIMA_results = SARIMA_model.fit()

print(SARIMA_results.summary().tables[0])
print(SARIMA_results.summary().tables[1])

pred_dynamic_SARIMA = SARIMA_results.get_prediction(start=pd.to_datetime('2012-01-01'), dynamic=True, full_results=True)
pred99_SARIMA = SARIMA_results.get_forecast(steps=len(test), alpha=0.1)

truck_sales_forecasted = pred_dynamic_SARIMA.predicted_mean
testCopy_SARIMA = test.copy()
testCopy_SARIMA['truck_sales_forecasted'] = np.power(10, pred99_SARIMA.predicted_mean)

testCopy_SARIMA

mse_SARIMA = ((testCopy_SARIMA['truck_sales'] - testCopy_SARIMA['truck_sales_forecasted']) ** 2).mean()
rmse_SARIMA = np.sqrt(mse_SARIMA)
print('The Root Mean Squared Error of SARIMA is {}'.format(round(rmse_SARIMA, 4)))

axis = train['truck_sales'].plot(label='Обучение', figsize=(15, 5))
testCopy_SARIMA['truck_sales'].plot(ax=axis, label='Тест', alpha=0.7)
testCopy_SARIMA['truck_sales_forecasted'].plot(ax=axis, label='Предсказано', alpha=0.7)
axis.set_xlabel('Даты')
axis.set_ylabel('Продажи грузовиков')
plt.legend(loc='best')
plt.show()
plt.close()

#XGBoost Time Series

target_XGB = "truck_sales"
features_XGB = ["sales_year","sales_month"]

X_train_XGB = train[features_XGB]
y_train_XGB = train[target_XGB]

X_test_XGB = test[features_XGB]
y_test_XGB = test[target_XGB]

import xgboost as xgb
model_XGB = xgb.XGBRegressor(base_score=0.5, booster='gbtree',
                       n_estimators=1000,
                       early_stopping_rounds=50,
                       objective='reg:linear',
                       max_depth=3,
                       learning_rate=0.01)
model_XGB.fit(X_train_XGB, y_train_XGB,
        eval_set=[(X_train_XGB, y_train_XGB), (X_test_XGB, y_test_XGB)],
        verbose=100)

feature_importance_XGB = pd.DataFrame(data=model_XGB.feature_importances_,
             index=model_XGB.feature_names_in_,
             columns=['importance'])
feature_importance_XGB.sort_values('importance').plot(kind='barh', title='Важность признаков')
plt.show()

predictions_XGBR = model_XGBR.predict(X_test_XGB)
predictions_XGBR

test.loc[:,"predictions"] = predictions_XGBR
df_XGBR = df_truck.merge(test[["predictions"]], how='left', right_index=True, left_index=True)
ax = df_XGBR["truck_sales"].plot(figsize=(15,5))
df_XGBR["predictions"].plot(ax=ax, style='-')
plt.legend(['Truth Data', 'Predictions'])
ax.set_title('Raw Dat and Prediction')
plt.show()

rmse_XGB = np.sqrt(mean_squared_error(test['truck_sales'], test['predictions']))
print(f'RMSE Score of XGBRegressor: {rmse_XGB:0.2f}')

# LSTM
!pip install -q tensorflow

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Подготовьте данные для модели LSTM:
# Преобразуйте данные в массив NumPy
data_values_LSTM = df_truck['truck_sales'].values.reshape(-1, 1)

# Нормализуйте данные (масштабируйте в диапазон [0, 1])
scaler = MinMaxScaler()
data_scaled_LSTM = scaler.fit_transform(data_values_LSTM)

# Разделите данные на обучающую и тестовую выборки
train_size_LSTM = int(len(data_scaled_LSTM) * 0.8)
train_data_LSTM = data_scaled_LSTM[:train_size_LSTM]
test_data_LSTM = data_scaled_LSTM[train_size_LSTM:]

# Функция для создания последовательных данных для модели LSTM
def create_sequences(data, seq_length):
    sequences = []
    targets = []
    for i in range(len(data) - seq_length):
        sequence = data[i:i+seq_length]
        target = data[i+seq_length]
        sequences.append(sequence)
        targets.append(target)
    return np.array(sequences), np.array(targets)

# Определите длину последовательности и создайте последовательные данные
seq_length_LSTM = 12  # Например, используйте данные за последний год (12 месяцев)
X_train_LSTM, y_train_LSTM = create_sequences(train_data_LSTM, seq_length_LSTM)
X_test_LSTM, y_test_LSTM = create_sequences(test_data_LSTM, seq_length_LSTM)

# Создайте и обучите модель LSTM
model_LSTM = Sequential()
model_LSTM.add(LSTM(50, activation='relu', input_shape=(seq_length_LSTM, 1)))
model_LSTM.add(Dense(1))
model_LSTM.compile(optimizer='adam', loss='mean_squared_error')
model_LSTM.fit(X_train_LSTM, y_train_LSTM, epochs=100, batch_size=32)

# Прогнозирование на тестовых данных
y_pred_LSTM = model_LSTM.predict(X_test_LSTM)

# Выполните инверсию масштабирования, чтобы получить исходные значения
y_pred_inv_LSTM = scaler.inverse_transform(y_pred_LSTM)
y_test_inv_LSTM = scaler.inverse_transform(y_test_LSTM.reshape(-1, 1))

# Визуализируйте результаты прогноза
plt.figure(figsize=(12, 6))
plt.plot(df_truck.index[-len(y_test_inv_LSTM):], y_test_inv_LSTM, label='Факт', marker='o')
plt.plot(df_truck.index[-len(y_test_inv_LSTM):], y_pred_inv_LSTM, label='Прогноз', linestyle='--')
plt.title("Прогноз продаж месячных авиабилетов с использованием LSTM")
plt.xlabel("Месяц")
plt.ylabel("Количество продаж")
plt.legend()
plt.grid(True)
plt.show()

# Оценка точности прогноза
mae_LSTM = mean_absolute_error(y_test_inv_LSTM, y_pred_inv_LSTM)
mse_LSTM = mean_squared_error(y_test_inv_LSTM, y_pred_inv_LSTM)
rmse_LSTM = np.sqrt(mse_LSTM)

print(f"Mean Absolute Error (MAE) for LSTM: {mae_LSTM:.3f}")
print(f"Mean Squared Error (MSE) for _LSTM: {mse_LSTM:.3f}")
print(f"Root Mean Squared Error (RMSE) for LSTM: {rmse_LSTM:.3f}")

#RandomForestRegressor
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Добавляем временные признаки (например, лаги), чтобы модель учитывала прошлые значения временного ряда.
df_RFR=df_truck.copy()
df_RFR['Lag1'] = df_RFR['truck_sales'].shift(1)
df_RFR['Lag2'] = df_RFR['truck_sales'].shift(2)
df_RFR.dropna(inplace=True)

# Разделение данных на обучающие и тестовые выборки
train_size_RFR = int(len(df_RFR) * 0.8)
train_data_RFR = df_RFR.iloc[:train_size_RFR]
test_data_RFR = df_RFR.iloc[train_size_RFR:]

#Создание и обучение модели случайного леса с временными признаками
model_RFR = RandomForestRegressor(n_estimators=100)
model_RFR.fit(train_data_RFR[['Lag1', 'Lag2']], train_data_RFR['truck_sales'])

# Выполнение прогноза на тестовой выборке
test_data_RFR['Predictions'] = model_RFR.predict(test_data_RFR[['Lag1', 'Lag2']])

# Визуализация прогноза
plt.figure(figsize=(12, 6))
plt.plot(train_data_RFR.index, train_data_RFR['truck_sales'], label='Обучение')
plt.plot(test_data_RFR.index, test_data_RFR['truck_sales'], label='Тест')
plt.plot(test_data_RFR.index, test_data_RFR['Predictions'], color='red', label='Прогноз')
plt.title("Прогнозирование продаж грузовиков")
plt.xlabel("Дата")
plt.ylabel("Количество продаж")
plt.legend()
plt.grid(True)
plt.show()

# Оценка точности прогноза
mse_RFR = mean_squared_error(test_data_RFR['truck_sales'], test_data_RFR['Predictions'])
mae_RFR = mean_absolute_error(test_data_RFR['truck_sales'], test_data_RFR['Predictions'])
rmse_RFR = np.sqrt(mse_RFR)

print(f"Mean Absolute Error (MAE) для RandomForestRegressor: {mae_RFR:.3f}")
print(f"Mean Squared Error (MSE) для RandomForestRegressor: {mse_RFR:.3f}")
print(f"Root Mean Squared Error (RMSE) для RandomForestRegressor: {rmse_RFR:.3f}")

feature_importance_RFR = pd.DataFrame(data=model_RFR.feature_importances_,
             index=model_RFR.feature_names_in_,
             columns=['importance'])
feature_importance_RFR.sort_values('importance').plot(kind='barh', title='Важность признаков')
plt.show()

#ИТОГИ
models = ["ARMA_Model", "SARIMA_model", "XGBRegressor", "LSTM", "RandomForestRegressor"]
rmse = [rmse_ARMA, rmse_SARIMA, rmse_XGB, rmse_LSTM, rmse_RFR]

plt.figure(figsize=(10, 6))
bars = sns.barplot(x=models, y=rmse, hue=models, palette="coolwarm", edgecolor="black", linewidth=1.5, legend=False)

# Adding text labels
for bar in bars.patches:
    bars.annotate(f"{bar.get_height():.3f}",
                  (bar.get_x() + bar.get_width() / 2, bar.get_height()),
                   ha="center", va="bottom", fontsize=12, fontweight="bold", color="black")

plt.title("Сравнение производительности моделей (RMSE)", fontsize=14, fontweight="bold")
plt.ylabel("RMSE", fontsize=12)
plt.xticks(fontsize=11, fontweight="bold", rotation=15)
plt.ylim(0, 300)
plt.grid(axis="y", linestyle="--", alpha=0.5)
plt.show()

# Summary Table
summary_df = pd.DataFrame({
    "Model": models,
    "RMSE_score": rmse
})
summary_df = summary_df.sort_values(by="RMSE_score", ascending=True)
print("Сравнение моделей (по RMSE):\n")
print(summary_df.to_string(index=False))

# Print the best model
best_model = summary_df.iloc[0]["Model"]
print(f"\nЛучшая модель: {best_model} с RMSE равным {summary_df.iloc[0]['RMSE_score']:.3f}")